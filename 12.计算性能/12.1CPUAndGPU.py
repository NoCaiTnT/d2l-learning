# 提升CPU的利用率1
#   在计算a+b之前, 需要准备数据
#       主内存 -> L3 -> L2 -> L1 寄存器(数据只有在寄存器中才能参与运算)
#           L1访问延时: 0.5ns
#           L2访问延时: 7ns(14×L1)
#           主内存访问延时: 100ns(200×L1)
#   提升空间和时间的内存本地性(局部性原理)
#       时间: 重用数据使得保持他们在缓存里
#       空间: 按序读写数据使得可以预读取
# 样例分析
#   如果一个矩阵是按行存储, 访问一行会比访问一列要快
#       CPU一次读取64字节(缓存线)
#       CPU会"聪明的"提前读取下一个(缓存线)
# 提升CPU利用率2
#   高端CPU有几十个核
#   并行来利用所有核
#       超线程不一定提升性能, 因为他们共享寄存器
# 样例分析
#   左边比右边慢(python)
#       for i in range(len(a)):         c = a + b
#           c[i] = a[i] + b[i]
#       左边调用n次函数, 每次调用有开销
#       右边很容易被并行
# CPU vs GPU
#   核        6/64                   2K/4K
#   TFLOPS    0.2/1                 10/100
#   内存大小   32GB/1TB              16GB/32GB
#   内存带宽   30GB/s / 100GB/s    400GB/s / 1TB/s
#   控制流     强                     弱
# 提升GPU利用率
#   并行: 使用数千个线程
#   内存本地性: 缓存更小, 架构更加简单
#   少用控制语句
#       支持有限
#       同步开销很大
# CPU/GPU带宽
#   不要频繁在CPU和GPU之前传数据: 带宽限制, 同步开销
# 更多的CPUs和GPUs
#   CPU: AMD, ARM
#   GPU: AMD, Intel, ARM, Qualcomm...
# CPU/GPU高性能计算编程
#   CPU: C++或任何高性能语言: 编译器成熟
#   GPU
#       Nvidia上用CUDA: 编译器和驱动成熟
#       其他用OpenCL: 质量取决于硬件厂商
# 总结
#   CPU: 可以处理通用计算, 性能优化考虑数据读写效率和多线程
#   GPU: 使用更多的小核和更好的内存带宽, 适合能大规模并行的计算任务
