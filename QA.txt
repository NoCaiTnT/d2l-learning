04 数据操作+数据预处理
    1. reshape()和view()的区别
        reshape()函数返回一个新的张量，而view()函数返回一个与原始张量共享存储空间的张量。这意味着，当你使用reshape()函数改变张量形状时，会创建一个新的张量对象，而原始张量对象不会改变。而当你使用view()函数改变张量形状时，会返回一个新的张量对象，但是它与原始张量对象共享存储空间，因此对新张量的修改也会影响原始张量。
        reshape()函数可以处理任意形状的张量，而view()函数只能处理连续的张量。如果你尝试使用view()函数处理非连续的张量，会引发RuntimeError异常。
        reshape()函数可以自动推断某些维度的大小，而view()函数需要手动指定所有维度的大小。如果你使用reshape()函数时只指定了部分维度的大小，它会自动推断其他维度的大小。而如果你使用view()函数时没有指定所有维度的大小，会引发RuntimeError异常。
        reshape()函数可以使用-1作为占位符来自动计算某个维度的大小，而view()函数不支持使用-1作为占位符。
        reshape()和view()函数都是用于改变张量形状的函数，但是它们之间有一些区别。如果你需要处理非连续的张量或者需要自动推断某些维度的大小，应该使用reshape()函数。如果你需要处理连续的张量并且需要手动指定所有维度的大小，应该使用view()函数。
    2. pytorch中的tensor和numpy中array的区别
        tensor (张量) 承载了数学上的概念；array (数组) 是计算机的概念；但是本质上没有太大区别，可以简单理解为一个东西。
        在 NumPy 中，数组的类型是 numpy.ndarray。要获取数组的类型，通常会使用 type(x)，其中 x 是数组。在 PyTorch 中，一个张量的类型是 torch.Tensor 或其派生类型（如 torch.FloatTensor）。要获取张量的数据类型（如 float32, int64 等），可以使用 x.dtype，而 x.type() 可以用来获取完整的类型信息（包括其属于哪个类，如 torch.FloatTensor）。
        NumPy 数组默认只能在 CPU 上进行操作和计算。NumPy 本身不支持 GPU 加速。PyTorch 张量可以在 CPU 或 GPU 上进行操作和计算。这是 PyTorch 特别适合于深度学习和高性能计算的原因之一。通过使用 .to() 方法或 .cuda() 方法，可以将张量移动到 GPU 上（如果有可用的 GPU），这可以显著加速某些类型的计算。

07 自动求导
    1. 显示构造和隐式构造的区别
        显示构造：把整个计算图都先设计完成，然后再给值(先给公式再给值)
        隐式构造：就直接写程序流程，然后框架会在后台进行计算图的构建（先给值再给公式）
        实际使用过程中，显示构造计算图会麻烦很多！
    2. 为什么深度学习中一般对标量求导而不是对矩阵或者向量？
        因为loss是一个标量，如果loss是一个向量，那么向量对于矩阵的导数就是一个矩阵，矩阵再求导会变为4维矩阵，当网络结构比较深的时候，求导就会变为一个巨大的张量，导致无法计算
    3. 为什么获取.grad前需要backward
        计算梯度的计算开销比较大，因此计算机不会自动去计算梯度，因此在调用backward时才会进行梯度计算

08 线性回归+基础优化算法
    1. 线性回归为什么使用平方损失而不是绝对差值
        绝对差值在0点不可导
    2. 损失为什么要求平均
        平均后梯度会在数值上不会太大也不会太小，不除以n的话可以把学习率除以n。不管批量大小多大，计算的梯度总是差不多的，方便调学习率。
    3. gd梯度下降-sgd随机梯度下降怎么找到合适的学习率
        找一个对lr不敏感的算法，合理的参数初始化 --数值稳定性
    4. batchsize大小会影响最终模型结果吗，过小会导致最终累积的梯度计算不准确吗？
        过小好，过大不行。 batchsize越小对收敛越好，越小噪音越大，一定噪音对神经网络是个好事情，更鲁棒泛化性更好。
    5. 为什么机器学习优化算法都采用梯度下降（一阶求导算法），而不用牛顿法（二阶求导发），收敛速度更快，一般能算出一阶导，二阶导也应该能算。
        高阶导数的计算复杂度高，内存（显存）占用量大，也有可能hessian阵不可逆。
    6. data_iter写法，每次都把所有输入load进去，数据太多内存会爆掉。
	    数据太大，不能先load进内存，几百M数据load进去也可以。实际生成数据在硬盘里，每次读几个batch的数据进内存。
    7. 如果样本大小不是批量数的整倍数，会随机剔除多余的数据吗？
        拿到小批次样本，扔掉小批次，从别的批次（下一个epoch）随机取数据补全

10 多层感知机+代码实现
    1. 网络的一层是什么
        有多少层权重就有多少层
    2. 为什么增加神经元隐藏层的层数而不是神经元的个数。神经元万有近似性质？
        深度学习【好训练 每一层都学一点简单的东西，更深容易找到一个比较好的解】 or 宽度学习【不好训练，容易overfit过拟合，每个神经元都是一个并行的关系，并不能很好的进行特征的提取。】 优化函数解决不了太宽的问题。
    3. MLP和SVM的区别
        对于大数据量的问题，一般都会有神经网络解决，因为神经网络可以灵活编程；而SVM基于核，超参数设置单一，并且适用于小数据问题
    4. MLP和SVM的优缺点
        MLP的优点：
            强大的表达能力：MLP具有多层非线性变换，可以学习和表示复杂的非线性函数关系，适用于各种复杂的模式识别和回归任务。
            端到端学习：MLP可以通过端到端的训练，直接从原始数据中学习特征和表示，不需要手动设计特征工程。
            适应性强：MLP可以处理各种类型的数据，包括结构化数据、图像、文本等，并且可以适应不同规模的数据集和问题复杂度。
        MLP的缺点：
            训练时间长：MLP通常需要大量的数据和计算资源进行训练，尤其是在深度模型中，训练时间可能会很长。
            过拟合风险高：由于其强大的表达能力和大量的参数，MLP容易过拟合训练数据，特别是在数据量较少或者噪声较多的情况下。
            超参数调节困难：MLP有许多超参数需要调节，包括隐藏层的数量和大小、激活函数的选择、学习率等，调参过程可能较为繁琐和耗时。
        SVM的优点：
            较好的泛化性能：SVM通过最大化间隔来选择决策边界，通常具有较好的泛化能力，对于处理小样本、高维数据和非线性问题效果较好。
            解释性强：SVM的决策边界可以直观地解释为特征空间中的超平面，提供了对模型的理解和解释。
            不依赖全局最优：SVM的优化目标是凸优化问题，因此可以保证找到全局最优解，不受局部最优解的影响。
        SVM的缺点：
            计算开销大：在大规模数据集上，SVM的计算开销可能会很大，尤其是需要显式计算核矩阵时。
            只适用于二分类问题：传统的SVM模型只适用于二分类问题，对于多分类问题需要使用一些技巧或扩展。
            核函数选择困难：选择适当的核函数是SVM中的一个关键问题，不同的核函数可能对模型的性能产生较大影响，但核函数的选择通常需要通过试验和经验确定。

11 模型选择+过拟合和欠拟合
    1. svm缺点，神经网络相比
        通过kernel匹配模型复杂度，计算困难，很难做到100w的数据量。神经网络可以做大数据量的计算。第二：svm能调的东西不多，可调性不好。
        神经网络优点：神经网络本身是一种语言，一个个的小语句，不同layer是一个个不同的小工具，一句句的连起来，通过神经网络语言编程，来描述整个物体或者整个世界或要解决问题的理解。比较不太直观，但是可编程性好的框架。可以做到很大的数据集，卷积可以很好的做特征的提取。理论上单层隐藏层MLP能拟合所有的数据集，实际上训练不出来。所以要有一个比较好的结构，尽量帮助拟合。例如cnn本身就是MLP，只是对一些weight固定住了，告诉神经网络我觉得数据有空间信息，要这样去处理空间信息。RNN告诉有时序信息，要这样去训练。用神经网络来描述对问题的理解，帮助训练。
    2. SVM和神经网络的优缺点
        SVM如上
        神经网络的优点：
            强大的表达能力：神经网络具有多层非线性变换，可以学习和表示复杂的非线性函数关系，适用于各种复杂的模式识别和回归任务。
            端到端学习：神经网络可以通过端到端的训练，直接从原始数据中学习特征和表示，不需要手动设计特征工程。
            适应性强：神经网络可以处理各种类型的数据，包括结构化数据、图像、文本等，并且可以适应不同规模的数据集和问题复杂度。
        神经网络的缺点：
            训练时间长：神经网络通常需要大量的数据和计算资源进行训练，尤其是在深度模型中，训练时间可能会很长。
            过拟合风险高：由于其强大的表达能力和大量的参数，神经网络容易过拟合训练数据，特别是在数据量较少或者噪声较多的情况下。
            超参数调节困难：神经网络有许多超参数需要调节，包括隐藏层的数量和大小、激活函数的选择、学习率等，调参过程可能较为繁琐和耗时。
            黑箱模型：神经网络通常被视为黑盒模型，难以理解模型内部的决策过程，解释性较差。
    3. 如何有效设计超参数【优化 optuna】
        网格搜索，所有遍历一次（开销太大）。
        贝叶斯方法计算次数多。【一百一千一万次效果才会好一点】
        超参数设计：一靠自己的经验；二靠自己调参，判断测试结果；三靠随机，随机参数训练从中选一个效果比较好的。
        Optuna 是一个用于超参数优化的自动化调参库，它专门用于优化机器学习模型的超参数，以提高模型的性能和效果。Optuna 提供了一种高效的方法来搜索超参数空间，以找到最佳的超参数组合，从而优化模型的性能并提高模型在测试数据上的表现。
        在深度学习中，模型的性能往往受到超参数的影响。例如，学习率、批量大小、层数、隐藏单元数量等超参数都会直接影响模型的训练速度、收敛性和泛化能力。通过使用 Optuna 进行超参数优化，可以更快速地找到最佳的超参数组合，从而提高深度学习模型的性能和效果。
        总而言之，Optuna 在深度学习中表示一种用于自动化超参数优化的工具，能够帮助用户更高效地优化模型的性能，并提升模型的泛化能力和表现。
    4. 对于二分类问题，实际情况是1:9， 训练集两种类型数据的比例应该是1:1还是1:9？
        最好验证集上两类数据分布差不多，避免模型偏好数据占比较大的那一类，也可加权重避免。（趋向于1:1比较好）（复制或者加权）
    5. k折交叉验证的目的？还需要使用这个超参数训练一遍吗？
        k折交叉验证目的就是为了确定超参数，一种最常见做法再用这个超参数训练一次模型；第二不再训练模型，找到K折里精度最好【或随便】的一折，选择该参数模型，代价就是模型少看了一些训练集；第三种把K个模型全部拿下来，所有模型都预测一次然后求均值，代价是计算成本高，但是增加了模型稳定性。
    6. 为什么SVM打败了MLP，深度学习又打败了SVM？
        svm简单，不用咋调参，有数学理论，有人推，就是多层感知机要流行起来。深度学习虽没有理论，但是实际效果好，就更流行了。发展的问题。深度学习并没有改变机器学习核心的东西：要搞数据，避免overfitting，要看误差。
    7. 模型容量是什么？
        指模型能够拟合函数的能力
    8.为什么使用同样模型结构和训练数据，只是随机初始化不同，最后集成都一定会好？
        原因是降低了模型的方差（variance）
        模型是统计模型，优化是数值优化。【统计学、优化】最后的模型=统计学模型【模型定义】+怎么做的优化的结果。统计学模型一样的基础上，随机初始化数值不一样，最后结果不一样。每个模型都有一定的偏移，但是方差【噪音】每次优化，做n个模型取均值降低方差，可能效果比较好。

12 权重衰退
    1. 为什么参数不过大模型复杂度就低呢？
        限制整个模型在优化的时候，是在很小的范围取参数。只能在在一些比较平滑的模型曲线上取参数，这样就学不出一个很复杂的模型。
    2. 实践中权重衰减的值一般设置为多少好？有时感觉权重衰减的效果并不好
        一般取1e-2, 1e-3， 1e-4 (0.01, 0.001, 0.0001)。 权重衰退有一点点，但是不要太指望这个方法。如果模型很复杂，权重衰退没有很好的效果。可以试下 1e-3，效果不好换别的方法。
    3. 为什么要把w往小的拉？如果最优解的w就是比较大的数，那权重衰减是不是会有反作用？
        数据是有噪音的，可能学到的不是真正的最优解，lambda过大过小都会和最优解离得比较远，所以选择的lambda值要合适。

13 丢弃法dropout (一般丢弃概率为0.1, 0.5, 0.9)
    1. dropout随机置0对求梯度和反向传播的影响是什么？
        被丢弃的权重不会更新, 对于求梯度是一个对称的
    2. 丢弃的依据是什么？
        相当于正则化, 参数过大, 模型还是过拟合, 参数过小, 模型欠拟合
    3. 在使用BN的时候还有必要使用dropout吗？
        BN是用于卷积神经网络, 而dropout基本只用于全连接层(不会用于卷积层)

14 数值稳定性+模型初始化
    1. 为什么激活函数σ(w^t h^(t-1))对输入h^(t-1)求导是对角矩阵？
        https://blog.csdn.net/m0_51133942/article/details/135365992
        因为σ(w^t h^(t-1))对输入h^(t-1)求导相当于f(x)对x求导, 而f(x)和x又都是向量, 因此求导结果为矩阵, 而f(x)关于x的导数只有x对应的时候才有值, 其余为0, 因此矩阵只有对角线有值, 即为对角矩阵
    2. nan, inf是怎么产生以及怎么解决
        产生:
            inf是学习率调的太大, 或者权重初始值太大
            nan一般是除以0, 在梯度很小的时候除了一个0(学习率过高, 梯度爆炸, 数值不稳定的激活函数, 数据预处理问题)
        解决:
            合理的初始化权重(均值为0, 方差区间取小一点, 然后慢慢调大, 使得训练有进展), 激活函数不要选错, 学习率不要选的太大(一直往小的调, 直到不出现)

16 PyTorch神经网络基础
    1. 使用one-hot(独热编码)时, 列数过多导致内存炸掉, 怎么解决?
        A. 使用稀疏矩阵进行存储
        B. 不使用ont-hot, 使用别的方法
        C. 忽略掉这一列特征

19 卷积层
    1. 为什么要权重变形? 为什么要重新索引?
        是从全连接层到卷积层的关系, 即从全连接层通过如何限制权重来变换到卷积层, 卷积层是一个特殊的全连接层
    2. 为什么不应该看那么远?感受野不是越大越好吗?
        计算成本增加, 空间信息丢失(关注全局而丢失局部), 过拟合
        类似于全连接层(每一层太宽不如多加深几层), 每一层看的太宽不如多几层来看
    3. 1乘1的卷积的作用?
        1. 通道数变换, 可以升维或者降维
        2. 进行特征融合, 对输入特征图的每个通道进行加权求和
        3. 减少计算复杂度和参数量, 在大型卷积核（例如3x3或5x5）之前使用1x1卷积可以显著减少计算复杂度和参数量

20 卷积层里的填充和步幅
    1. 核大小, 填充, 步幅这三个超参数的重要程度排序
        填充通常为核大小-1, 步幅通常为1(不为1的情况是计算量太大, 通常取2), 核大小是最关键的
    2. 为什么用3×3的卷积核, 不是很小吗?
        对于第一层来讲, 3×3的视野确实很小, 但对于顶层, 一个像素可能对应了第一层的很多像素, 从而获取了很大的视野

22 池化层
    1. 池化层的作用?为什么用的越来越少?
        两个作用: 减少计算量(步长为2), 让卷积对位置没有那么敏感
        原因:
            可以在卷积中设置步长缩小输出
            会对数据本身进行增强, 使得卷积不会过拟合到某个位置, 淡化了池化层的作用

23 经典卷积神经网络LeNet
    1. 池化和卷积是不是更适用于图像数据, 而对于时序性数据(做分类)是不是不适用?
        卷积可以做时序性数据(一维卷积), 池化可以用也可以不用
    2. 卷积层的通道数的增加表示信息被放大了吗?
        一般来讲高宽减半, 通道数加2倍, 卷积是信息损失, 多个通道是这一个像素的不同特征, 拓宽了信息
    3. 查看卷积神经网络到底学到了什么?
        https://poloclub.github.io/cnn-explainer/
    4. 卷积神经网络是做结构化数据还是非结构化数据?
        卷积神将网络本身是结构化的
        深度学习是通过一个结构化的网络来抽取结构化数据/非结构化数据的信息

24 深度卷积神经网络
    1. 网络要求输入的size是固定的, 实际图片不一定是这个size, 强行resize会不会效果变差?
        通常做法是将短边resize到256, 在中间再抠出来224×224, 或者随机扣除5个224×224, 多做几次共同预测, 保持横纵比在里面抠图, 扣成输入需要的大小

26 网络中的网络NiN
    1. 为什么最后在全局平均池化层后面没有softmax?
        交叉熵损失里面已经包括了softmax, 因此在网络中不需要做

28 批量归一化BN
    1. xavier和BN的区别
        本质上没有区别, 是一个思想, 就是模型的参数稳定了, 收敛就不会慢
        xavier初始化是说在初始的时候, 选取和合适的初始化参数能够使得模型稳定, 但是不能保证之后
        BN层则是在训练的时候, 在每一层都进行一次"归一化"
    2. BN是不是和权重衰退的作用类似？
        权重衰退可以认为是每次更新权重的时候都除了一个小值(就是不要让权重的变化范围太大)
        但是BN却不会太多的影响前一层的权重
    3. BN能用在MLP中吗？
        可以的, 但是如果网络不深的话其实作用不大
        一般都是网络较深的时候才会出现上层更新快, 下层更新慢的问题
        也就是BN层一般用于深层网络，浅层的MLP加上BN效果不会见得很棒
    4. Layer Norm 和 BN的区别
        https://blog.csdn.net/Little_White_9/article/details/123345062?spm=1001.2014.3001.5506

34 多GPU训练
    1. resnet中的卷积层能否全替换成mlp来实现很深的mlp网络?
        换成全连接会导致巨大的参数量, 从而过拟合
    2. 验证集准确率震荡较大是哪个参数影响最大?
        学习率
    3. 为了让底层的网络能够训练, 使用BN, 为什么不采用不同层使用不同的学习率?
        可以, 但是学习率不好调, 比如前几层要比后几层大多少等等
    4. 不同型号的GPU影响深度学习的性能吗?
        需要按照GPU之间的性能来分配样本, 例如GPU0的性能是GPU1的2倍, 则给GPU0分配的样本数也应该是GPU1的2倍, 以此来保证两个GPU同时计算完梯度等等
    5. 为什么batch size越大, 训练的有效性(收敛速度)会下降?
        batch size大到一定程度, 每个batch内的样本的多样性不会比之前有多大增长, 对梯度的贡献也不会比之前的batch大多少, 但是大的batch size会带来更多的训练时间, 就造成了训练有效性下降
        假设总共有1_0000个样本(假设是batch size取100或者1000时得到的梯度是差不多的, 即上述当batch size大到一定程度时, 多样性不会增加太多)
            1. 如果一个batch是100, 那么一个epoch可以迭代100次梯度;
            2. 如果一个batch是1000, 那么一个epoch只能迭代10次梯度, 如果想要收敛, 意味着需要更多epoch.

36 数据增广
    1. 样本数量足够多, 是不是就不需要增广?
        和样本的多样性相关, 多样性足够大就可以不做增广, 但一般场景下数据多样性比较有限

40. 狗的种类识别
    1. 为什么在测试集做图像增广时, 要先resize到256再中心剪裁到224, 而不是直接resize到224?
        历史遗留问题, 之前的模型都是这样
    2. 图像增广是对变换后的图片进行训练吗?
        是的, 对原始图片进行图像增广, 使用增广后的图片去训练, 数量不变

42. 锚框
    1. 每次做NMS时, 是只针对相同类别做循环过滤, 还是针对不同类别都做过滤
        两种都可以

45. SSD
    1. 多个Loss相加, 可能有一个很大, 另一个很小, 这样小的损失就拿不到太多梯度, 该怎么办?
        加权

47. 转置卷积
    1. 转置卷积的含义
        卷积是从图像中提取特征, 但是会将图片缩小, 但我们要预测图像中每一个像素的类别, 因此需要通过反卷积将图像放缩回原来的大小, 类似于encoder-decoder
        特征放大
        反卷积的意义是卷积之后的矩阵的每个元素有一个感受视野, 反卷积希望通过这个元素还原感受视野里面的内容, 并且由于卷积后的元素的感受视野有相交的情况,
        所以反卷积中也出现了结果中有些元素的值来源于卷积结果的一个或多个元素的现象, 理论上通过反卷积, 我们可以通过将特征图反卷积还原到原图大小从而获取到我们的卷积核在原图中提取的是什么信息

    2. 上采样, 还可以使用线性插值, 这两种方式哪个更好?
        转置卷积可以实现插值, 使图像变大, 但其可以学习更多的东西
        可以通过线性插值初始化转置卷积的核

48. 全连接卷积神经网络FCN
    1. 为什么使用双线性插值?
        只是提供一个不错的权重初始值, 使用随机问题也不大, 只是可能需要多迭代几轮

51. 序列模型
    1. 马尔可夫假设中, τ是不是越大越好?
        τ越大, 观察到的数据越多, 效果比较好
        然而有局限性, 当τ太大时, 训练样本就会减少, 而且需要更强(更复杂)的模型来拟合这些大量的数据

52. 文本预处理
    1. 为什么要根据频率来进行排序?
        符合局部性原理, 频率高的词汇更有可能出现在一起, 从而更好的拟合模型

55. 循环神经网络的实现
    1. 批量随不随机取, 是否是看批量之间有没有时间关联?
        有关联也可以随机取, 但是不随机取的话, 可以更好的保持时间关联, 不随机取会使随机性降低, 容易过拟合
        一般随机取, 因为RNN不足以记住很长的序列

56. 门控循环单元GRU
    1. R_t和Z_t的网络结构一样, 为什么可以自动地将二者区分为重置门和更新门?
        模型通过梯度下降自动学习权重, 也就是说, 通过梯度下降, 模型学习到了这个区分

