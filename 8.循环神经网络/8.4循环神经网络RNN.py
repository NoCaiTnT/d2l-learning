# 潜变量自回归模型
#   使用潜变量h_t总结过去信息
#   p(h_t|h_t-1, x_t-1) 当前的潜变量h_t是由上一个潜变量h_t-1和上一个值x_t-1决定的
#   p(x_t|h_t, x_t-1)   当前的值x_t是由当前的潜变量h_t和上一个值x_t-1决定的

# 循环神经网络
#                  好    世     界
#   输出      o     o     o     o
#            ↑     ↑     ↑     ↑
#   隐变量    h  →  h  →  h  →  h
#               ↗     ↗     ↗
#   观察      x     x     x     x
#            你    好     世    界
#   更新隐藏状态: h_t = φ(W_hh h_t-1 + W_hx x_t-1 + b_h)    去掉第一项就变退化成了MLP
#   输出: o_t = φ(W_ho h_t + b_o)

# 困惑度perplexity
#   衡量一个语言模型的好坏可以用平均交叉熵 π=1/n \sum_{i=1}^{n} -log p(x_t|x_t-1, ...)
#       p是语言模型的预测概率, x_t是真实词
#   历史原因NLP使用困惑度exp(π)来衡量, 是平均每次可能选项
#       1表示完美, 无穷大是最差情况

# 梯度裁剪
#   迭代中计算这T个时间步上的梯度, 在反向传播过程中产生长度为O(T)的矩阵乘法链, 导致数值不稳定
#   梯度裁剪能有效预防梯度爆炸
#       如果梯度长度超过θ, 那么将其变为θ
#       g ← min(1, θ/|g|) g

# 总结
#   循环神经网络的输出取决于当下输入和前一时间的隐变量
#   应用到语言模型中时, 循环神经网络根据当前词预测下一时刻词
#   通常使用困惑度来衡量语言模型的好坏